# üåæ Agriculture Yield Prediction System

-----------------------------------------

 PROJECT DESCRIPTION

This project is an end-to-end Data Engineering pipeline built to analyze agriculture crop yield data using cloud data warehouse and big data tools.

The aim of this project is to:

‚Ä¢ Clean and transform raw agriculture data  
‚Ä¢ Store data in Snowflake Data Warehouse  
‚Ä¢ Perform advanced SQL analysis  
‚Ä¢ Process large datasets using Apache Spark  
‚Ä¢ Implement data optimization and security  
‚Ä¢ Visualize insights using Power BI Dashboard  

-----------------------------------------

TOOLS & TECHNOLOGIES USED

Python          - Data Cleaning (ETL)
Apache Spark    - Batch Processing
Snowflake       - Cloud Data Warehouse
SQL             - Data Analysis
Power BI        - Dashboard
Draw.io         - Architecture Diagram
GitHub          - Project Repository

-----------------------------------------
PROJECT FOLDER STRUCTURE

Agriculture-Yield-Prediction-System
‚îÇ
‚îú‚îÄ‚îÄ data
‚îÇ   ‚îú‚îÄ‚îÄ raw
‚îÇ   ‚îî‚îÄ‚îÄ cleaned
‚îÇ
‚îú‚îÄ‚îÄ etl
‚îÇ   ‚îî‚îÄ‚îÄ etl.py
‚îÇ
‚îú‚îÄ‚îÄ spark
‚îÇ   ‚îî‚îÄ‚îÄ spark_job.py
‚îÇ
‚îú‚îÄ‚îÄ sql
‚îÇ   ‚îî‚îÄ‚îÄ snowflake_queries.sql
‚îÇ
‚îú‚îÄ‚îÄ architecture
‚îÇ   ‚îî‚îÄ‚îÄ architecture_diagram.png
‚îÇ
‚îú‚îÄ‚îÄ Agriculture_Yield_Dashboard.pbix
‚îî‚îÄ‚îÄ README.md

-----------------------------------------

‚öôÔ∏è PROJECT IMPLEMENTATION STEPS

STEP 1: DATA COLLECTION
Agriculture dataset is collected in CSV format containing:
Area, Crop Type, Year, Yield, Rainfall, Temperature, Pesticides usage.

-----------------------------------------

STEP 2: PYTHON ETL PIPELINE
Python (Pandas) is used to:
‚Ä¢ Remove unwanted columns  
‚Ä¢ Rename column names  
‚Ä¢ Handle missing values  
‚Ä¢ Clean dataset  

Cleaned dataset is saved as:
cleaned_data.csv

-----------------------------------------

STEP 3: DATA WAREHOUSE DESIGN (STAR SCHEMA)

Snowflake database is created with:

‚Ä¢ FACT_YIELD Table  
‚Ä¢ DIM_CROP Table  
‚Ä¢ DIM_LOCATION Table  
‚Ä¢ DIM_WEATHER Table  

-----------------------------------------

STEP 4: DATA LOADING IN SNOWFLAKE

Cleaned dataset is uploaded to Snowflake table:
CLEANED_DATA

-----------------------------------------

STEP 5: ADVANCED SQL ANALYSIS

Following SQL concepts are implemented:

‚Ä¢ CTE (Common Table Expressions)  
‚Ä¢ Window Functions  
‚Ä¢ Ranking  
‚Ä¢ Running Total  
‚Ä¢ Partitioning  

Used for:
Area-wise yield analysis  
Crop ranking  
Yield trends over years  

-----------------------------------------

STEP 6: APACHE SPARK PROCESSING

Apache Spark is used for:

‚Ä¢ Distributed batch processing  
‚Ä¢ Aggregation of large datasets  
‚Ä¢ Yield analysis by Area and Crop  

-----------------------------------------

STEP 7: SNOWFLAKE OPTIMIZATION

Storage optimization features used:

‚Ä¢ Clustering  
‚Ä¢ Time Travel  
‚Ä¢ Semi-Structured Data (VARIANT)  
‚Ä¢ Warehouse Scaling  

-----------------------------------------

STEP 8: SECURITY IMPLEMENTATION

Security is implemented using:

‚Ä¢ RBAC (Role Based Access Control)  
‚Ä¢ Data Masking Policy  

-----------------------------------------

STEP 9: DASHBOARD VISUALIZATION

Power BI dashboard is created for:

‚Ä¢ Area-wise Yield Analysis  
‚Ä¢ Crop-wise Yield Comparison  
‚Ä¢ Year-wise Trend Analysis  

-----------------------------------------

 ARCHITECTURE

Architecture diagram shows:

‚Ä¢ Data Source  
‚Ä¢ ETL Pipeline  
‚Ä¢ Snowflake Warehouse  
‚Ä¢ Optimization Layer  
‚Ä¢ Spark Processing  
‚Ä¢ Security Layer  
‚Ä¢ Power BI Dashboard  

-----------------------------------------

HOW TO RUN THE PROJECT:

Run ETL Script:
python etl/etl.py

Run Spark Job:
python spark/spark_job.py

Run SQL Queries:
Execute snowflake_queries.sql in Snowflake Worksheet

-----------------------------------------


OUTPUT

Interactive dashboard created in Power BI for agriculture yield insights.




